# report_generator.py
import os
import json
import openai
import hashlib
from pathlib import Path
from datetime import datetime

# LlamaIndexç›¸å…³æ¨¡å—
from llama_index.llms.openai import OpenAI
from llama_index.embeddings.openai import OpenAIEmbedding
from llama_index.core import Settings, StorageContext, load_index_from_storage, VectorStoreIndex
from llama_index.core.readers import SimpleDirectoryReader
from llama_index.core.tools import FunctionTool
from llama_index.core.agent import ReActAgent
from llama_index.core.node_parser import SimpleNodeParser

# ========== åˆå§‹åŒ– OpenAI å’Œ LlamaIndex ==========
openai.api_key = "sk-42kUEMoma40GUNhl595bE5D53e994eC59a7a469534564f11"
openai.base_url = "https://api.gptapi.us/v1"

Settings.llm = OpenAI(model="gpt-4o",
                      api_key=openai.api_key,
                      base_url=openai.base_url)
Settings.embed_model = OpenAIEmbedding(model="text-embedding-3-small")


# ========== å·¥å…·å‡½æ•° ==========
def compute_file_hash(file_path: Path) -> str:
    with open(file_path, "rb") as f:
        content = f.read()
    return hashlib.sha256(content).hexdigest()


def update_vuln_kb_index(index_dir: str, data_dir: str) -> VectorStoreIndex:
    current_files = {
        os.path.abspath(os.path.join(data_dir, f))
        for f in os.listdir(data_dir)
        if os.path.isfile(os.path.join(data_dir, f))
    }

    try:
        storage_context = StorageContext.from_defaults(persist_dir=index_dir)
        index = load_index_from_storage(storage_context)
        print("âœ… æˆåŠŸåŠ è½½å·²æœ‰ç´¢å¼•ã€‚")

        existing_file_paths = set()
        for node in storage_context.docstore.docs.values():
            file_path = node.metadata.get("file_path")
            if file_path:
                existing_file_paths.add(os.path.abspath(file_path))

        new_files = current_files - existing_file_paths
        deleted_files = existing_file_paths - current_files

        if new_files:
            new_documents = SimpleDirectoryReader(
                input_files=list(new_files)).load_data()
            new_nodes = SimpleNodeParser().get_nodes_from_documents(
                new_documents)
            index.insert_nodes(new_nodes)
            print(f"âœ… å·²æ›´æ–°æ–‡æ¡£æ•°ï¼š{len(new_files)}")

        if deleted_files:
            delete_ids = []
            for node_id, node in storage_context.docstore.docs.items():
                file_path = node.metadata.get("file_path")
                if file_path and os.path.abspath(file_path) in deleted_files:
                    delete_ids.append(node_id)
            if delete_ids:
                index.delete_nodes(delete_ids)
                print(f"ğŸ—‘ï¸ å·²åˆ é™¤å¤±æ•ˆèŠ‚ç‚¹æ•°ï¼š{len(delete_ids)}")

    except Exception as e:
        print(f"âš ï¸ ç´¢å¼•åŠ è½½å¤±è´¥ï¼Œæ­£åœ¨é‡å»ºï¼š{e}")
        documents = SimpleDirectoryReader(input_dir=data_dir).load_data()
        index = VectorStoreIndex.from_documents(documents)
        index.storage_context.persist(persist_dir=index_dir)
        print("âœ… ç´¢å¼•é‡å»ºå®Œæˆã€‚")

    index.storage_context.persist(persist_dir=index_dir)
    return index


# ========== åˆ›å»º Agent ==========
vuln_kb_index = update_vuln_kb_index(index_dir="./storage/vuln_kb",
                                     data_dir="./data/vuln_kb")
kb_engine = vuln_kb_index.as_query_engine(similarity_top_k=3)


def query_kb(query: str) -> str:
    response = kb_engine.query(query)
    answer = response.response.strip()

    references = []
    for i, node_with_score in enumerate(response.source_nodes):
        node = node_with_score.node
        file_path = node.metadata.get("file_path", "æœªçŸ¥æ–‡ä»¶")
        content = node.get_content().strip()
        references.append(f"ã€å¼•ç”¨{i+1}ã€‘æ–‡ä»¶ï¼š`{file_path}`\næ‘˜å½•å†…å®¹ï¼š\n> {content}")

    full_response = f"{answer}\n\nå¼•ç”¨å†…å®¹ï¼š\n" + "\n\n".join(references)
    return full_response


kb_tool = FunctionTool.from_defaults(
    fn=query_kb,
    name="vuln_knowledge_base",
    description="æä¾›æ¼æ´èƒŒæ™¯çŸ¥è¯†ï¼Œå¦‚åŸç†ã€å±å®³å’Œä¿®å¤å»ºè®®ã€‚è¾“å…¥æ˜¯æ¼æ´åç§°ï¼Œå¦‚ SQL æ³¨å…¥")

report_agent = ReActAgent.from_tools(tools=[kb_tool],
                                     llm=Settings.llm,
                                     verbose=True)


# ========== å¯¹å¤–æä¾›çš„æ ¸å¿ƒå‡½æ•° ==========
def generate_report(decision_data: dict) -> str:
    """
    è¾“å…¥ï¼šå†³ç­–JSONå­—å…¸
    è¾“å‡ºï¼šMarkdownæ ¼å¼çš„æ¼æ´æŠ¥å‘Šå­—ç¬¦ä¸²
    """
    input_desc = "ä¸‹é¢æ˜¯æ¼æ´æ£€æµ‹å™¨è¾“å‡ºçš„ç»“æœ JSONï¼š\n"
    input_desc += json.dumps(decision_data, ensure_ascii=False, indent=2)
    input_desc += "\n\nè¯·åˆ†æè¿™ä¸ª JSONï¼Œå¹¶æŒ‰å¦‚ä¸‹æ ¼å¼ç”ŸæˆmarkdownæŠ¥å‘Šï¼š\n"
    input_desc += """
### æ¼æ´å‡½æ•°åï¼š`<function_name>`
- **æ¼æ´ä½ç½®**ï¼š<location>
- **é£é™©ç­‰çº§**ï¼š<severity>
- **æ¼æ´ç±»å‹**ï¼š<vulnerability_type>
- **æ¼æ´æè¿°**ï¼š
  <ä¸­æ–‡çš„description>
- **ä¿®å¤å»ºè®®**ï¼š
  <å»ºè®®çš„ä¿®å¤æ–¹æ³•>
---
"""
    input_desc += "\nè¯·åŠ¡å¿…åœ¨æ¯æ¡æ¼æ´æŠ¥å‘Šä¸­å†™å‡ºä½ å¼•ç”¨çš„æ–‡ä»¶è·¯å¾„åŠå†…å®¹ç‰‡æ®µï¼Œç”¨å¦‚ä¸‹æ ¼å¼ï¼š\n"
    input_desc += "æ¥è‡ªæ–‡ä»¶ï¼š`<file_path>`\nå†…å®¹èŠ‚é€‰ï¼š\n> è¢«å¼•ç”¨çš„å†…å®¹\n"

    result = report_agent.chat(input_desc)
    return result.response.strip()
